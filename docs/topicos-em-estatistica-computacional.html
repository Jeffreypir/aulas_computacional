<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Tópicos em Estatística Computacional | ESTATÍSTICA COMPUTACIONAL</title>
  <meta name="description" content="Disciplina de Estatística Computacional - Departamento de Estatística" />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Tópicos em Estatística Computacional | ESTATÍSTICA COMPUTACIONAL" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://prdm0.github.io/aulas_computacional/" />
  
  <meta property="og:description" content="Disciplina de Estatística Computacional - Departamento de Estatística" />
  <meta name="github-repo" content="prdm0/aulas_computacional" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Tópicos em Estatística Computacional | ESTATÍSTICA COMPUTACIONAL" />
  
  <meta name="twitter:description" content="Disciplina de Estatística Computacional - Departamento de Estatística" />
  

<meta name="author" content="Docente: Prof. Dr. Pedro Rafael Diniz Marinho   E-mail: pedro.rafael.marinho@gmail.com / pedro@de.ufpb.br" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="r-miscelanea-e-topicos-avancados.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/d3-3.3.8/d3.min.js"></script>
<script src="libs/dagre-0.4.0/dagre-d3.min.js"></script>
<link href="libs/mermaid-0.3.0/dist/mermaid.css" rel="stylesheet" />
<script src="libs/mermaid-0.3.0/dist/mermaid.slim.min.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/chromatography-0.1/chromatography.js"></script>
<script src="libs/DiagrammeR-binding-1.0.1/DiagrammeR.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Início</a></li>
<li class="chapter" data-level="" data-path="prefacio.html"><a href="prefacio.html"><i class="fa fa-check"></i>Prefácio</a><ul>
<li class="chapter" data-level="" data-path="prefacio.html"><a href="prefacio.html#tecnologias-abordadas-no-curso"><i class="fa fa-check"></i>Tecnologias abordadas no curso</a></li>
<li class="chapter" data-level="" data-path="prefacio.html"><a href="prefacio.html#teorias-abordadas-no-curso"><i class="fa fa-check"></i>Teorias abordadas no curso</a></li>
<li class="chapter" data-level="" data-path="prefacio.html"><a href="prefacio.html#sugestoes-de-passos-para-revisao-da-linguagem-r"><i class="fa fa-check"></i>Sugestões de passos para revisão da linguagem R</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="revisao-basica-da-linguagem-r.html"><a href="revisao-basica-da-linguagem-r.html"><i class="fa fa-check"></i><b>1</b> Revisão básica da linguagem R</a><ul>
<li class="chapter" data-level="" data-path="revisao-basica-da-linguagem-r.html"><a href="revisao-basica-da-linguagem-r.html#exercicios-propostos"><i class="fa fa-check"></i>Exercícios propostos</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="o-evangelho-do-gnulinux.html"><a href="o-evangelho-do-gnulinux.html"><i class="fa fa-check"></i><b>2</b> O Evangelho do GNU/Linux</a><ul>
<li class="chapter" data-level="2.1" data-path="o-evangelho-do-gnulinux.html"><a href="o-evangelho-do-gnulinux.html#um-breve-historico"><i class="fa fa-check"></i><b>2.1</b> Um breve histórico</a></li>
<li class="chapter" data-level="2.2" data-path="o-evangelho-do-gnulinux.html"><a href="o-evangelho-do-gnulinux.html#vantagens-em-utilizar-gnulinux"><i class="fa fa-check"></i><b>2.2</b> Vantagens em utilizar GNU/Linux</a></li>
<li class="chapter" data-level="2.3" data-path="o-evangelho-do-gnulinux.html"><a href="o-evangelho-do-gnulinux.html#algumas-distribuicoes"><i class="fa fa-check"></i><b>2.3</b> Algumas distribuições</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="versionamento-de-codigo.html"><a href="versionamento-de-codigo.html"><i class="fa fa-check"></i><b>3</b> Versionamento de código</a><ul>
<li class="chapter" data-level="3.1" data-path="versionamento-de-codigo.html"><a href="versionamento-de-codigo.html#git"><i class="fa fa-check"></i><b>3.1</b> Git</a></li>
<li class="chapter" data-level="3.2" data-path="versionamento-de-codigo.html"><a href="versionamento-de-codigo.html#github"><i class="fa fa-check"></i><b>3.2</b> GitHub</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="relatorio-markdown-em-r.html"><a href="relatorio-markdown-em-r.html"><i class="fa fa-check"></i><b>4</b> Relatório Markdown em R</a><ul>
<li class="chapter" data-level="4.1" data-path="relatorio-markdown-em-r.html"><a href="relatorio-markdown-em-r.html#markdown"><i class="fa fa-check"></i><b>4.1</b> Markdown</a><ul>
<li class="chapter" data-level="4.1.1" data-path="relatorio-markdown-em-r.html"><a href="relatorio-markdown-em-r.html#sintaxe"><i class="fa fa-check"></i><b>4.1.1</b> Sintaxe</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="relatorio-markdown-em-r.html"><a href="relatorio-markdown-em-r.html#r-markdown"><i class="fa fa-check"></i><b>4.2</b> R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html"><i class="fa fa-check"></i><b>5</b> R - miscelânea e tópicos avançados</a><ul>
<li class="chapter" data-level="5.1" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#operador---pipe"><i class="fa fa-check"></i><b>5.1</b> Operador %&gt;% - Pipe</a><ul>
<li class="chapter" data-level="" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#exercicios-1"><i class="fa fa-check"></i>Exercícios</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#funcoes"><i class="fa fa-check"></i><b>5.2</b> Funções</a><ul>
<li class="chapter" data-level="5.2.1" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#passando-atributos"><i class="fa fa-check"></i><b>5.2.1</b> Passando atributos</a></li>
<li class="chapter" data-level="5.2.2" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#funcoes-anonimas"><i class="fa fa-check"></i><b>5.2.2</b> Funções anônimas</a></li>
<li class="chapter" data-level="5.2.3" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#escopo-lexico"><i class="fa fa-check"></i><b>5.2.3</b> Escopo léxico</a></li>
<li class="chapter" data-level="5.2.4" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#avaliacao-preguicosa"><i class="fa fa-check"></i><b>5.2.4</b> Avaliação preguiçosa</a></li>
<li class="chapter" data-level="5.2.5" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#varargs-dot-dot-dot"><i class="fa fa-check"></i><b>5.2.5</b> varargs: … (dot-dot-dot)</a></li>
<li class="chapter" data-level="5.2.6" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#funcoes-infixas"><i class="fa fa-check"></i><b>5.2.6</b> Funções infixas</a></li>
<li class="chapter" data-level="5.2.7" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#funcao-de-substituicao"><i class="fa fa-check"></i><b>5.2.7</b> Função de substituição</a></li>
<li class="chapter" data-level="5.2.8" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#closures"><i class="fa fa-check"></i><b>5.2.8</b> Closures</a></li>
<li class="chapter" data-level="" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#exercicios-2"><i class="fa fa-check"></i>Exercícios</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#funcionais"><i class="fa fa-check"></i><b>5.3</b> Funcionais</a><ul>
<li class="chapter" data-level="5.3.1" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#funcionais-do-r-base"><i class="fa fa-check"></i><b>5.3.1</b> Funcionais do R Base</a></li>
<li class="chapter" data-level="5.3.2" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#funcionais-do-pacote-purrr"><i class="fa fa-check"></i><b>5.3.2</b> Funcionais do pacote purrr</a></li>
<li class="chapter" data-level="" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#exercicios-3"><i class="fa fa-check"></i>Exercícios</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#sistema-s3"><i class="fa fa-check"></i><b>5.4</b> Sistema S3</a><ul>
<li class="chapter" data-level="5.4.1" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#classes"><i class="fa fa-check"></i><b>5.4.1</b> Classes</a></li>
<li class="chapter" data-level="5.4.2" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#criando-funcoes-genericas"><i class="fa fa-check"></i><b>5.4.2</b> Criando funções genéricas</a></li>
<li class="chapter" data-level="" data-path="r-miscelanea-e-topicos-avancados.html"><a href="r-miscelanea-e-topicos-avancados.html#exercicio"><i class="fa fa-check"></i>Exercício</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="topicos-em-estatistica-computacional.html"><a href="topicos-em-estatistica-computacional.html"><i class="fa fa-check"></i><b>6</b> Tópicos em Estatística Computacional</a><ul>
<li class="chapter" data-level="6.1" data-path="topicos-em-estatistica-computacional.html"><a href="topicos-em-estatistica-computacional.html#geracao-de-numeros-pseudo-aleatorios"><i class="fa fa-check"></i><b>6.1</b> Geração de Números Pseudo-Aleatórios</a><ul>
<li class="chapter" data-level="6.1.1" data-path="topicos-em-estatistica-computacional.html"><a href="topicos-em-estatistica-computacional.html#metodo-da-transformacao-inversa-caso-discreto"><i class="fa fa-check"></i><b>6.1.1</b> Método da Transformação Inversa (Caso Discreto)</a></li>
<li class="chapter" data-level="6.1.2" data-path="topicos-em-estatistica-computacional.html"><a href="topicos-em-estatistica-computacional.html#metodo-da-aceitacao-e-rejeicao"><i class="fa fa-check"></i><b>6.1.2</b> Método da aceitação e rejeição</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="topicos-em-estatistica-computacional.html"><a href="topicos-em-estatistica-computacional.html#exercicio-1"><i class="fa fa-check"></i>Exercício</a></li>
<li class="chapter" data-level="6.2" data-path="topicos-em-estatistica-computacional.html"><a href="topicos-em-estatistica-computacional.html#otimizacao-nao-linear"><i class="fa fa-check"></i><b>6.2</b> Otimização Não-Linear</a><ul>
<li class="chapter" data-level="6.2.1" data-path="topicos-em-estatistica-computacional.html"><a href="topicos-em-estatistica-computacional.html#metodos-gradiente"><i class="fa fa-check"></i><b>6.2.1</b> Metódos Gradiente</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ESTATÍSTICA COMPUTACIONAL</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="topicos-em-estatistica-computacional" class="section level1">
<h1><span class="header-section-number">6</span> Tópicos em Estatística Computacional</h1>
<div id="geracao-de-numeros-pseudo-aleatorios" class="section level2">
<h2><span class="header-section-number">6.1</span> Geração de Números Pseudo-Aleatórios</h2>
<p>O conteúdo para esse tópico entra-se em PDF e poderá ser acessado <a href="files/computacional.pdf"><strong>aqui</strong></a>. Em um futuro próximo, essa seção será reescrita e fará parte do corpo deste HTML.</p>
<div id="metodo-da-transformacao-inversa-caso-discreto" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Método da Transformação Inversa (Caso Discreto)</h3>
<p>O método da transformação inversa também poderá ser aplicado também para gerar observações de v.a.’s discretas. Seja <span class="math inline">\(X\)</span> uma v.a. discreta, tal que</p>
<p><span class="math display">\[... &lt; x_{i-1} &lt; x_i &lt; x_{i+1} &lt;\, ...\,.\]</span>
As observações acima são pontos de descontinuidade de <span class="math inline">\(F_X(x)\)</span>. Então, a transformação inversa é <span class="math inline">\(F_X^{-1} = x_i\)</span>, quando <span class="math inline">\(F_X(x_{i-1}) &lt; u \leq F_X(x_i)\)</span>. O algoritmo que segue poderá ser utilizado para gerar observações de <span class="math inline">\(X\)</span>.</p>
<p><strong>Algoritmo</strong>:</p>
<ol style="list-style-type: decimal">
<li>Gere um número pseudo-leatório <span class="math inline">\(u\)</span> de uma v.a. <span class="math inline">\(U \sim \mathcal{U}(0,1)\)</span>;</li>
<li>Retorne <span class="math inline">\(x_i\)</span> como observação de <span class="math inline">\(X\)</span>, tal que <span class="math inline">\(F(x_{i-1}) &lt; u \leq F(x_i)\)</span>.</li>
</ol>
<p>Em outras palavras, gere <span class="math inline">\(u\)</span> de uma v.a. <span class="math inline">\(U \sim \mathcal{U}(0,1)\)</span> e compare na sequência:</p>
<ol style="list-style-type: decimal">
<li>Se <span class="math inline">\(u &lt; p_0\)</span>, faça <span class="math inline">\(X = x_0\)</span> e pare;</li>
<li>Se <span class="math inline">\(u &lt; p_0 + p_1\)</span>, faça <span class="math inline">\(X = x_1\)</span> e pare;</li>
<li>Se <span class="math inline">\(u &lt; p_0 + p_1 + p_2\)</span>, faça <span class="math inline">\(X = x_2\)</span> e pare;</li>
<li>etc;</li>
</ol>
<p>em que <span class="math inline">\(p_j = P(X = j)\)</span>.</p>
<p><strong>Exemplo</strong>: Implemente, em R, uma função que retorna a quantidade de observações de uma v.a. <span class="math inline">\(X\)</span> com função de probabilidade:</p>
<p><span class="math display">\[P(X = 1) = 0.3, P(X = 2) = 0.2, P(X = 3) = 0.35, P(X = 4) = 0.15.\]</span></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="topicos-em-estatistica-computacional.html#cb1-1"></a>random &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">n =</span> 1L){</span>
<span id="cb1-2"><a href="topicos-em-estatistica-computacional.html#cb1-2"></a>  x &lt;-<span class="st"> </span>1L<span class="op">:</span>4L</span>
<span id="cb1-3"><a href="topicos-em-estatistica-computacional.html#cb1-3"></a>  probs &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.2</span>, <span class="fl">0.35</span>, <span class="fl">0.15</span>)</span>
<span id="cb1-4"><a href="topicos-em-estatistica-computacional.html#cb1-4"></a>  </span>
<span id="cb1-5"><a href="topicos-em-estatistica-computacional.html#cb1-5"></a>  u &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n =</span> n, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">1</span>)</span>
<span id="cb1-6"><a href="topicos-em-estatistica-computacional.html#cb1-6"></a>  </span>
<span id="cb1-7"><a href="topicos-em-estatistica-computacional.html#cb1-7"></a>  <span class="co"># Criando uma função para ser passada à um funcional.</span></span>
<span id="cb1-8"><a href="topicos-em-estatistica-computacional.html#cb1-8"></a>  comp &lt;-<span class="st"> </span><span class="cf">function</span>(u){</span>
<span id="cb1-9"><a href="topicos-em-estatistica-computacional.html#cb1-9"></a>    <span class="co"># Retorna a primeira ocorrência de TRUE</span></span>
<span id="cb1-10"><a href="topicos-em-estatistica-computacional.html#cb1-10"></a>    <span class="kw">match</span>(<span class="ot">TRUE</span>, u <span class="op">&lt;</span><span class="st"> </span><span class="kw">cumsum</span>(probs))</span>
<span id="cb1-11"><a href="topicos-em-estatistica-computacional.html#cb1-11"></a>  }</span>
<span id="cb1-12"><a href="topicos-em-estatistica-computacional.html#cb1-12"></a>  </span>
<span id="cb1-13"><a href="topicos-em-estatistica-computacional.html#cb1-13"></a>  purrr<span class="op">::</span><span class="kw">map_dbl</span>(<span class="dt">.x =</span> u, <span class="dt">.f =</span> <span class="op">~</span><span class="st"> </span><span class="kw">comp</span>(.x))</span>
<span id="cb1-14"><a href="topicos-em-estatistica-computacional.html#cb1-14"></a>  </span>
<span id="cb1-15"><a href="topicos-em-estatistica-computacional.html#cb1-15"></a>}</span>
<span id="cb1-16"><a href="topicos-em-estatistica-computacional.html#cb1-16"></a><span class="kw">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb1-17"><a href="topicos-em-estatistica-computacional.html#cb1-17"></a><span class="kw">random</span>(<span class="dt">n =</span> 10L)</span></code></pre></div>
<pre><code>##  [1] 4 1 2 3 4 1 4 4 3 3</code></pre>
<p>Em um estudo de simulação, poderemos ter interesse em gerar observações equiprováveis, em que a v.a. <span class="math inline">\(X\)</span> assume um número finito de observações, tal forma que:</p>
<p><span class="math display">\[P(X = j) = \frac{1}{n}, \,\, j = 1, 2, \ldots, n.\]</span>
Porém, note que para esse caso, não precisaremos fazer muitas comparações, uma vez que sabemos que <span class="math inline">\(x = j\)</span> quando <span class="math inline">\(u \leq \frac{j-1}{n}\)</span>. Sendo assim, tomamos <span class="math inline">\(x = j\)</span> quando <span class="math inline">\(nu \leq j - 1\)</span>.</p>
<p>Note que fazer <span class="math inline">\(x = j\)</span> quando <span class="math inline">\(nu \leq j - 1\)</span> equivale a fazer <span class="math inline">\(x = \mathrm{Int}(nu) + 1\)</span>, em que <span class="math inline">\(\mathrm{Int}(\cdot)\)</span> retorna a parte inteira de um número.</p>
<p><strong>Exemplo</strong>: Seja <span class="math inline">\(X \sim Bernoulli(p)\)</span>, em que <span class="math inline">\(P(X = 0) = 1 - p\)</span> e <span class="math inline">\(P(X = 1) = p\)</span>, com <span class="math inline">\(0\leq p \leq 1\)</span>. A função <code>rbernoulli(n = 1L, p)</code> retorna possíveis observações de <span class="math inline">\(X\)</span>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="topicos-em-estatistica-computacional.html#cb3-1"></a>rbernoulli &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">n =</span> 1L, p){</span>
<span id="cb3-2"><a href="topicos-em-estatistica-computacional.html#cb3-2"></a>  u &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n =</span> n, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">1</span>)</span>
<span id="cb3-3"><a href="topicos-em-estatistica-computacional.html#cb3-3"></a>  cond &lt;-<span class="st"> </span>(u <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p)</span>
<span id="cb3-4"><a href="topicos-em-estatistica-computacional.html#cb3-4"></a>  comp &lt;-<span class="st"> </span><span class="cf">function</span>(x){</span>
<span id="cb3-5"><a href="topicos-em-estatistica-computacional.html#cb3-5"></a>    <span class="kw">ifelse</span>(x <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p, 0L, 1L)</span>
<span id="cb3-6"><a href="topicos-em-estatistica-computacional.html#cb3-6"></a>  }</span>
<span id="cb3-7"><a href="topicos-em-estatistica-computacional.html#cb3-7"></a>  purrr<span class="op">::</span><span class="kw">map_int</span>(<span class="dt">.x =</span> u, <span class="dt">.f =</span> <span class="op">~</span><span class="st"> </span><span class="kw">comp</span>(.x))</span>
<span id="cb3-8"><a href="topicos-em-estatistica-computacional.html#cb3-8"></a>}</span>
<span id="cb3-9"><a href="topicos-em-estatistica-computacional.html#cb3-9"></a><span class="kw">set.seed</span>(<span class="dv">0</span>) <span class="co"># Fixando uma semente.</span></span>
<span id="cb3-10"><a href="topicos-em-estatistica-computacional.html#cb3-10"></a>n &lt;-<span class="st"> </span><span class="fl">1e4</span> <span class="co"># Número de observações.</span></span>
<span id="cb3-11"><a href="topicos-em-estatistica-computacional.html#cb3-11"></a>result &lt;-<span class="st"> </span><span class="kw">rbernoulli</span>(<span class="dt">n =</span> n, <span class="dt">p =</span> <span class="fl">0.6</span>) <span class="co"># prob.  de sucesso = 0.6.</span></span>
<span id="cb3-12"><a href="topicos-em-estatistica-computacional.html#cb3-12"></a><span class="co"># Probabilidade de sucesso aproximada.</span></span>
<span id="cb3-13"><a href="topicos-em-estatistica-computacional.html#cb3-13"></a><span class="kw">sum</span>(result <span class="op">==</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span>n</span></code></pre></div>
<pre><code>## [1] 0.5973</code></pre>
</div>
<div id="metodo-da-aceitacao-e-rejeicao" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Método da aceitação e rejeição</h3>
<p>Em situações em que não podemos fazer uso do método da inversão (impossível obter a função quantílica) e nem conhecemos uma transformação que envolve uma variável aleatória ao qual sebemos gerar observações, poderemos fazer uso do <strong>método a aceitação e rejeição</strong>.</p>
<p>Suponha que <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são variáveis aleatórias com função densidade de probabilidade (fdp) ou função de probabilidade (fp) <span class="math inline">\(f\)</span> e <span class="math inline">\(g\)</span>, respectivamente. Além disso, suponha que existe uma constante <span class="math inline">\(c\)</span> de tal forma que</p>
<p><span class="math display">\[\frac{f(t)}{g(t)} \leq c,\]</span>
para todo valor de <span class="math inline">\(t\)</span>, com <span class="math inline">\(f(t) &gt; 0\)</span>. Para utilizar o método a aceitação e rejeição para gerar observações da v.a. <span class="math inline">\(X\)</span>, utilizando o algoritmo mais abaixo, antes, encontre uma v.a. <span class="math inline">\(Y\)</span> com pdf ou fp <span class="math inline">\(g\)</span>, tal que satisfaça a condição acima.</p>
<p><strong>Importante</strong>:</p>

<div class="rmdimportant">
<div class="text-justify">
<p>É importante que a v.a. <span class="math inline">\(Y\)</span> escolhida seja de tal forma que você consiga gerar facilmente suas observações. Isso se deve ao fato do método da aceitação e rejeição ser computacionalmente mais intensivo que métodos mais diretos como o método da transformação ou o método da inversão que exige apenas a regração de números pseudo-aleatórios com distribuição uniforme.</p>
</div>
</div>

<p><strong>Algoritmo do Método da Aceitação e Rejeição</strong>:</p>
<p>1 - Gere uma observação <span class="math inline">\(y\)</span> proveniente de uma v.a. <span class="math inline">\(Y\)</span> com fdp/fp <span class="math inline">\(g\)</span>;</p>
<p>2 - Gere uma observação <span class="math inline">\(u\)</span> de uma v.a. <span class="math inline">\(U\sim \mathcal{U} (0, 1)\)</span>;</p>
<p>3 - Se <span class="math inline">\(u &lt; \frac{f(y)}{cg(y)}\)</span> aceite <span class="math inline">\(x = y\)</span>; caso contrário rejeite <span class="math inline">\(y\)</span> como observação da v.a. <span class="math inline">\(X\)</span> e volte para o passo anterior.</p>
<p><strong>Prova</strong>: Consideremos o caso discreto, ou seja, que <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são v.a.s com fp’s <span class="math inline">\(f\)</span> e <span class="math inline">\(g\)</span>, respectivamente. Pelo passo 3 do algoritmo acima, temos que <span class="math inline">\(\{aceitar\} = \{x = y\} = u &lt; \frac{f(y)}{cg(y)}\)</span>. Isto é,</p>
<p><span class="math display">\[P(aceitar | Y = y) = \frac{P(aceitar \cap \{Y = y\})}{g(y)} = \frac{P(U \leq f(y)/cg(y)) \times g(y)}{g(y)} = \frac{f(y)}{cg(y)}.\]</span>
Daí, pelo <a href="https://pt.wikipedia.org/wiki/Lei_da_probabilidade_total"><strong>Teorema da Probabilidade Total</strong></a>, temos que:</p>
<p><span class="math display">\[P(aceitar) = \sum_y P(aceitar|Y=y)\times P(Y=y) = \sum_y \frac{f(y)}{cg(y)}\times g(y) = \frac{1}{c}.\]</span>
Portanto, pelo método da aceitação e rejeição aceitamos ocorrência de <span class="math inline">\(Y\)</span> como sendo uma ocorrência de <span class="math inline">\(X\)</span> com probabilidade <span class="math inline">\(1/c\)</span>. Além disso, pelo Teorema de Bayes, temos que</p>
<p><span class="math display">\[P(Y = y | aceitar) = \frac{P(aceitar|Y = y)\times g(y)}{P(aceitar)} = \frac{[f(y)/cg(y)] \times g(y)}{1/c} = f(y).\]</span>
O resultado logo acima, mostra que aceitar <span class="math inline">\(x = y\)</span> pelo procedimento do algoritmo equivale a aceitar um valor proveniente de <span class="math inline">\(X\)</span> que tem fp <span class="math inline">\(f\)</span>. Para o caso contínuo, a demonstração é similar.</p>
<p><strong>Importante</strong>:</p>

<div class="rmdimportant">
<div class="text-justify">
<p>Perceba que para reduzir o custo computacional do método, deveremos escolher <span class="math inline">\(c\)</span> de tal forma que possamos maximizar <span class="math inline">\(P(aceitar)\)</span>. Sendo assim, escolher um valor exageradamente grande da constante <span class="math inline">\(c\)</span> irá reduzir a probabilidade de aceitar uma observação de <span class="math inline">\(Y\)</span> como sendo observação da v.a. <span class="math inline">\(X\)</span>.</p>
</div>
</div>

<p><strong>Nota</strong>:</p>

<div class="rmdnote">
<div class="text-justify">
<p>Computacionalmente, é conveniente considerar <span class="math inline">\(Y\)</span> como sendo uma v.a. com distribuição uniforme no suporte de <span class="math inline">\(f\)</span>, uma vez que gerar observações de uma distribuição uniforme é algo simples em qualquer computador. Para o caso discreto, considerar <span class="math inline">\(Y\)</span> com distribuição uniforme discreta poderá ser uma boa alternativa.</p>
</div>
</div>

</div>
</div>
<div id="exercicio-1" class="section level2 unnumbered">
<h2>Exercício</h2>
<ol style="list-style-type: decimal">
<li><p>Quais as propriedades de um bom gerador de números pseudo-aleatórios? Disserte sobre cada uma delas.</p></li>
<li><p>Implemente o gerador <strong>Midsquare</strong> idealizado pelo matemático John von Neumann. Por que o gerador <strong>Midsquare</strong> não é um bom gerador? Explique.</p></li>
<li><p>Defina matematicamente o gerador congruencial linear. Implemente uma função em R que implementa esse gerador.</p></li>
<li><p>O gerador <strong>Randu</strong> é definido por <span class="math inline">\(x_{i + 1} = 65539 \times x_i\,\mathrm{mod}\,31\)</span>. O <strong>Randu</strong> é um gerador congruencial misto ou multiplicativo?</p></li>
<li><p>Por que o gerador <strong>Randu</strong> é um dos peiores geradores de números pseudo-aleatório já criado? Explique.</p></li>
<li><p>Defina um gerador congruencial de período completo para geração de números pseudo-aleatórios com distribuição uniforme no intervalo <span class="math inline">\((0,1)\)</span></p></li>
<li><p>Explique o método da transformação para geração de números pseudo aleatório. Apresente um exemplo.</p></li>
<li><p>Defina o método da inversão para geração de números pseudo-aleatórios. Sempre será possível utilizar esse método? Explique.</p></li>
<li><p>Implemente uma função para geração de números pseudo-aleatórios com distribuição normal padrão. A função deverá implementar o método de Box-Müller e o método polar. Ao final obtenha um histograma com os números gerados (mil valores) e realize um teste de normalidade. Realize um teste de normalidade.</p></li>
<li><p>Seja <span class="math inline">\(X\)</span> uma variável aleatória em um espaço de probabilidade <span class="math inline">\((\Omega, \mathcal{A},\mathcal{P})\)</span> e suponha que <span class="math inline">\(X \sim \mathcal{U}(0,1)\)</span>. Obtenha a distribuição de <span class="math inline">\(Y = -\log(X)\)</span>.</p></li>
<li><p>Com base na distribuição da variável aleatória (v.a.) <span class="math inline">\(Y\)</span> do exercício acima, implemente uma função em R que gere observações de <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Conhecendo a distribuição da v.a. <span class="math inline">\(X\)</span>, implemente para cada um dos itens que seguem, uma função para geração de observações da v.a. <span class="math inline">\(Y\)</span>:</p>
<ul>
<li><p><span class="math inline">\(X \sim \mathrm{Exp}(\lambda),\)</span> com <span class="math inline">\(x \geq 0\)</span> e <span class="math inline">\(\lambda &gt; 0\)</span> e <span class="math inline">\(Y = \sum_{i = 1}^n X_i \sim \Gamma(n, \lambda)\)</span>;</p></li>
<li><p><span class="math inline">\(X \sim \mathrm{Exp}(\lambda)\)</span>, com <span class="math inline">\(x \geq 0\)</span> e <span class="math inline">\(\lambda&gt;0\)</span> e <span class="math inline">\(Y = \mu - \beta\log(\lambda X) \sim \mathrm{Gumbel}(\mu,\beta)\)</span>, com <span class="math inline">\(\mu \in \Bbb{R}\)</span> e <span class="math inline">\(\beta&gt;0\)</span>;</p></li>
<li><p><span class="math inline">\(X \sim \mathcal{U}(0,1)\)</span> (contínua) e <span class="math inline">\(Y = m + s[-\log(X)]^{-1/\alpha} \sim\)</span> Fréchet<span class="math inline">\((\alpha,s,m)\)</span>, com <span class="math inline">\(x, \alpha,s &gt; 0\)</span> e <span class="math inline">\(m \in \Bbb{R}\)</span>.</p></li>
</ul></li>
<li><p>Cite algumas das propriedades do gerador <strong>Mersenne Twister</strong>. Qual o seu período de ocorrência?</p></li>
<li><p>Explique o algoritmo do método da aceitação e rejeição.</p></li>
<li><p>Utilizando o método da aceitação e rejeição, implemente uma função que gere valores aleatório provenientes da distribuição da v.a. <span class="math inline">\(X\)</span> tal que</p></li>
</ol>
<p><span class="math display">\[P(X = 1) = 0.3, P(X = 2) = 0.2, P(X = 3) = 0.35, P(X = 4) = 0.15.\]</span></p>
<ol start="16" style="list-style-type: decimal">
<li>Implemente duas funções que geram observações da v.a. <span class="math inline">\(X\)</span> utilizando o método da transformação para v.a. discretas e pelo método da aceitação e rejeição. Qual método é mais eficiente computacionalmente? Por que?</li>
</ol>
<p><span class="math display">\[\begin{eqnarray}
P(X = 0) &amp;=&amp; 0.01, P(X = 1) = 0.04, P(X = 2) = 0.12,\nonumber\\
P(X = 3) &amp;=&amp; 0.27, P(X = 4) = 0.44, P(X = 5) = 0.62, \nonumber\\
P(X = 6) &amp;=&amp; 0.76, P(X = 7) = 0.87, P(X = 8) = 0.93, \nonumber\\
P(X = 9) &amp;=&amp; 0.97, P(X = 10) = 0.99, P(X = 11) = 0.99, \nonumber\\
P(X = 12) &amp;=&amp; 1.\nonumber
\end{eqnarray}\]</span></p>
<ol start="17" style="list-style-type: decimal">
<li><p>Implemente a função <code>rdisc(n = 1L, x, probs)</code> que retorna números pseudo-aleatórios de uma v.a. discreta <span class="math inline">\(X\)</span> que assume uma quantidade finita de observações. Os argumentos de <code>rdisc</code> estão especificados abaixo:</p>
<ul>
<li><p><code>n</code>: número de observações a serem geradas;</p></li>
<li><p><code>x</code>: vetor com as possíveis observações da v.a. <span class="math inline">\(X\)</span>;</p></li>
<li><p><code>probs</code>: vetor com as probabilidades das observações passadas à <code>x</code>.</p></li>
</ul></li>
<li><p>Implemente uma função para o método da aceitação e rejeição (<code>ar_fp(n, x, prob)</code>), para o caso em que deseja-se gerar observações de uma v.a. <span class="math inline">\(X\)</span> discreta. O argumento <code>n</code> refere-se à quantidade de observações a serem geradas, <code>x</code> é um vetor de valores assumidos por <span class="math inline">\(X\)</span> e <code>prob</code> é um vetor de probabilidades de cada observação de <span class="math inline">\(X\)</span>. A função <code>ar_fp(n, x, prob)</code> deverá escolher um valor adequado para <span class="math inline">\(c\)</span>.</p></li>
<li><p>Seja <span class="math inline">\(X\)</span> uma v.a. contínua com fdp <span class="math inline">\(f(x) = 6x(x-1)\)</span>, com <span class="math inline">\(0 &lt; x &lt; 1\)</span>. Implemente a função <code>rf(n, c = 1.5)</code> que gera números pseudo-aleatórios como observações de <span class="math inline">\(X\)</span>, pelo método da aceitação e rejeição, em que <code>n</code> é a quantidade de números a serem gerados e <code>c</code> é o valor da constante (0.5 por padrão) no algoritmo do método da aceitação e rejeição. <strong>Dica</strong>: considere <span class="math inline">\(Y \sim \mathcal{U}(0,1)\)</span>.</p></li>
</ol>
<!-- rf <- function(n = 1L, c = 6){ -->
<!--   f <- function(x) 6 * x * (1 - x) -->
<!--   i <- 1L -->
<!--   n_rejeicao <- 0L -->
<!--   vetor <- numeric(n) -->
<!--   while (i < n){ -->
<!--     u <- runif(n = 1L, min = 0, max = 1) -->
<!--     y <- runif(n = 1L, min = 0, max = 1) -->
<!--     if (u <= f(y)/c){ -->
<!--       vetor[i] <- y -->
<!--       i <- i + 1L -->
<!--     }  -->
<!--     else{ -->
<!--       n_rejeicao <- n_rejeicao + 1L -->
<!--     } -->
<!--   } -->
<!--   list(valores = vetor, rejeicao = n_rejeicao) -->
<!-- } -->
<!-- set.seed(0) -->
<!-- n <- 2e4 -->
<!-- c <- 20 -->
<!-- result <- rf(n = n, c = c) -->
<!-- # Probabilidade de aceitar: -->
<!-- 1/c -->
<!-- # Aproximacao da probabilidade de aceitar: -->
<!-- 1 - result$rejeicao/(n + result$rejeicao) -->
<ol start="20" style="list-style-type: decimal">
<li>Considerando a função <code>rf(n, c)</code> implementada no exercício anterior, quantos passos serão necessários para que possamos gerar 10 mil observações proveniente da distribuição de <span class="math inline">\(X\)</span>, considerando <code>c = 6</code>? Respectivamente, quantos passos serão necessários para serem gerados a mesma quantidade de observações de <span class="math inline">\(X\)</span> considerando <code>c = 0.5</code>? <strong>Dica</strong>: antes de chamar a função implementada, fixe a semente fazendo <code>set.seed(0)</code>.</li>
</ol>
</div>
<div id="otimizacao-nao-linear" class="section level2">
<h2><span class="header-section-number">6.2</span> Otimização Não-Linear</h2>
<p>Na estatísticas, em muitas simtuações práticas, temos o interesse de maximizar ou minimizar uma função objetivo. Por exemplo, na inferência esatística, é comum o nosso interesse na obtenção dos estimadores de máxima verossimilhança de parâmetros que indexam modelos ou distribuições de probabilidade. Um outro problema comum na estatística, mais precisamente na área de regressão é o de minimizar a soma dos quadrados de um conjunto de erros, em um modelo de regressão não-linear, por meio de mínimos quadrados não-lineares.</p>
<p><strong>Nota</strong>:</p>

<div class="rmdnote">
<div class="text-justify">
<p>Aqui, o termo <strong>otimizar</strong> estará se referindo à <strong>minimiar</strong> ou <strong>maximizar</strong> uma função objetivo. Dessa forma, por meio do contexto em que o termo esteja sendo utilizado, a ideia estará implícita. Além disso, por uma simples modificação na função objetivo, um algoritmo utilizado para maximizar uma função poderá ser convertido em um algoritmo para minimização de uma função.</p>
</div>
</div>

<p>Suponha que temos interesse em maximizar uma função objetivo, seja ela <span class="math inline">\(\psi(\pmb{\Theta}): \pmb{\Theta} \rightarrow \mathbb{R}\)</span>, em que <span class="math inline">\(\pmb{\Theta}\)</span> é um subspaço do <span class="math inline">\(\mathbb{R}^p\)</span>. Dessa forma, queremos encontrar o vetor <span class="math inline">\(\pmb{\theta}\)</span> (<span class="math inline">\(p \times 1\)</span>) que maximiza a função objetivo <span class="math inline">\(\psi(\cdot)\)</span>. Ou seja, queremos encontar</p>
<p><span class="math display">\[ \underset{\pmb{\theta}\, \in\, \pmb{\Theta}}{\mathrm{arg\,max}}\,\psi(\pmb{\theta}).\]</span></p>
<p>A maioria das situações práticas nos levam à problemas com a condição de primeira ordem,</p>
<p><span class="math display">\[\frac{\partial\,\psi(\pmb{\theta})}{\partial\,\pmb{\theta}} = \pmb{0},\]</span></p>
<p>resulta em um sistema de equações não-lineares que não apresenta solução em forma fechada. Nesses casos, a solução do problema de minimizar <span class="math inline">\(\psi(\pmb{\theta})\)</span> é obtida utilizando-se de métodos/algoritmos iterativos.</p>
<div id="metodos-gradiente" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Metódos Gradiente</h3>
<p>A classe de métodos mais utilizadas em situações em que a condição de primeira ordem resulta em um sistema não-lineare que não possue forma fechada é denominada de <strong>classe de métodos gradiente</strong>. Nessa classe de métodos, a atualização de <span class="math inline">\(\pmb{\theta}\)</span> para um vetor que mais se aproxima do ponto que minimiza <span class="math inline">\(\psi(\cdot)\)</span> (função objetivo) é dada de forma iterativa. Dessa forma, seja <span class="math inline">\(\pmb{\theta}_0\)</span> o ponto inicial (chute inicial) na <span class="math inline">\(t\)</span>-ésima iteração. Se o valor de <span class="math inline">\(\pmb{\theta}\)</span> que maximiza globalmente a função <span class="math inline">\(\psi(\cdot)\)</span> não tiver sido alcançado, calcula-se o vetor direcional (vetor gradiente) de <span class="math inline">\(\psi(\cdot)\)</span>, denotado aqui por <span class="math inline">\(\pmb\Delta_t\)</span> (<span class="math inline">\(p \times 1\)</span>) e o “tamanho do passo” <span class="math inline">\(\lambda_t\)</span>. Assim, o próximo valor de <span class="math inline">\(\pmb\theta\)</span> será atualizado para:</p>
<p><span class="math display">\[\pmb\theta_{t+1} = \pmb\theta_t + \lambda_t \pmb \Delta_t.\]</span></p>
<p><strong>Importante</strong>:</p>

<div class="rmdimportant">
<div class="text-justify">
<p>O vetor gradiente fornece a direção e o sentido de deslocamento, apartir de um ponto especificado, de subida que fornece um incremento em uma grandeza. No nosso caso, <span class="math inline">\(\pmb\Delta_t\)</span> (<span class="math inline">\(p \times 1\)</span>) irá indicar o sentido de subida, em que poderemos atualizar o valor de <span class="math inline">\(\pmb \theta\)</span> para um ponto que maximiza <span class="math inline">\(\psi(\cdot)\)</span>. Dessa forma, <span class="math inline">\(-\pmb\Delta_t\)</span> fornecerá a direção e o sentido de atualização de <span class="math inline">\(\pmb\theta\)</span> que nos levam à pontos que minimizam <span class="math inline">\(\psi(\cdot)\)</span>.</p>
</div>
</div>

<p>Note também que, em cada iteração do algoritmo, temos <span class="math inline">\(\pmb \theta_t\)</span> e <span class="math inline">\(\pmb \Delta_t\)</span> são conhecidos, uma vez que <span class="math inline">\(\pmb \theta_t\)</span> é o ponto <span class="math inline">\(\pmb \theta\)</span> atualizado na iteração <span class="math inline">\(t-1\)</span> e <span class="math inline">\(\pmb \Delta_t\)</span> é o vetor gradiente da função objetivo avaliado em <span class="math inline">\(\pmb \theta_t\)</span>.</p>
<p>Dado o conhecimento dessas quantidade, perceba que precisamos obter o “tamanho do passo” <span class="math inline">\(\lambda_t\)</span>. Assim, iremos recair em um problema de otimização secundário, denominado de <strong>procura em linha</strong>. Sendo assim, busca-se <span class="math inline">\(\lambda_t\)</span> de tal forma que</p>
<p><span class="math display">\[\frac{\partial \pmb\,\psi(\pmb \theta_t + \lambda_t\pmb\Delta_t)}{\partial \lambda_t} = \delta(\pmb \theta_t + \lambda_t\pmb\Delta_t)^{&#39;}\pmb \Delta_t = 0,\]</span>
em que <span class="math inline">\(\delta(\pmb \theta_t + \lambda_t\pmb\Delta_t)^{&#39;}\)</span> é o vetor gradiente tansposto (<span class="math inline">\(1 \times p\)</span>) da função objetivo avaliado no ponto <span class="math inline">\(\pmb \theta_t + \lambda_t\pmb\Delta_t.\)</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-8"></span>
<img src="topicos_estatistica_computacional_files/figure-html/unnamed-chunk-8-1.png" alt="Poblema de **procura em linha**, em que a função objetivo é função penas do tamanho do passo." width="672" />
<p class="caption">
Figura 6.1: Poblema de <strong>procura em linha</strong>, em que a função objetivo é função penas do tamanho do passo.
</p>
</div>
<p><strong>Observação</strong>:</p>

<div class="rmdobservation">
<div class="text-justify">
<p>É importante observar que o processo de busca em linha em um algoritmo de otimização não-linear tornará esse algoritmo computacionalmente intensivo. Dessa forma, muitos algoritmos adotam um conjunto de regras <em>ad hoc</em> que são computacionalmente menos custosas. Essa classe de algoritmos é o que denominamos de <strong>classe de métodos gradiente</strong>.</p>
</div>
</div>

<p>Na classe de métodos gradiente, fazemos</p>
<p><span class="math display">\[\pmb \Delta_t = M_t\delta_t,\]</span>
em que <span class="math inline">\(M_t\)</span> (<span class="math inline">\(p \times p\)</span>) é uma matriz positiva-definida e <span class="math inline">\(\delta_t\)</span> (<span class="math inline">\(p \times 1\)</span>) é o gradiente de <span class="math inline">\(\pmb\psi\)</span>, ambos na <span class="math inline">\(t\)</span>-ésima iteração. Para deixar claro, temos que <span class="math inline">\(\delta_t\)</span> = <span class="math inline">\(\delta_t(\pmb \theta_t) = \partial \, \psi(\pmb \theta_t)/\partial\, \pmb \theta_t\)</span>.</p>
<p>A motivação por trás dos métodos gradientes, ao em tomar <span class="math inline">\(\pmb \Delta_t = M_t\delta_t\)</span>, poderá ser entendida ao considerar uma aproximação para <span class="math inline">\(\psi(\pmb \theta_t + \lambda_t\pmb\Delta_t)\)</span> por uma <a href="https://pt.wikipedia.org/wiki/S%C3%A9rie_de_Taylor"><strong>série de Taylor</strong></a> de primeira ordem, em torno do ponto <span class="math inline">\(\lambda_t = 0\)</span>. Assim, temos que,</p>
<p><span class="math display">\[\psi(\pmb \theta_t + \lambda_t\pmb\Delta_t) \approx \psi(\pmb \theta_t) + \lambda_t \delta_t^{&#39;}\pmb \Delta_t,\]</span>
em que <span class="math inline">\(\delta_t^{&#39;}\)</span> é o vetor gradiente transposto (<span class="math inline">\(1 \times p\)</span>). Para reduzir a notação, tome <span class="math inline">\(\psi(\pmb \theta_t + \lambda_t\pmb\Delta_t) = \psi_{t+1}\)</span>. Assim, temos que</p>
<p><span class="math display">\[\psi_{t+1} - \psi_{t} \approx \lambda_t \delta_t^{&#39;}\pmb \Delta_t.\]</span></p>
<p>Assim, considerando <span class="math inline">\(\pmb \Delta_t = M_t\delta_t\)</span>, temos que</p>
<p><span class="math display">\[\psi_{t+1} - \psi_{t} \approx \lambda_t \delta_t^{&#39;}M_t\delta_t.\]</span></p>
<p>Para <span class="math inline">\(\delta_t\)</span> diferente de zero e <span class="math inline">\(\lambda_t\)</span> suficientemente pequeno, se <span class="math inline">\(\psi(\pmb\theta)\)</span> não assume o valor máximo da função, podemos sempre encontrar um tamanho de passo tal que uma iteração adicional no algoritmo irá incrementar o valor da função, ou seja, se aproximará um pouco mais do máximo global da função objetivo.</p>
<p>Perceba que sempre temos que <span class="math inline">\(\psi_{t+1} - \psi_{t} \geq 0\)</span>. Isso sempre será verdade, uma vez que <span class="math inline">\(M_t\)</span> é uma matriz positiva-definida, o que implica que <span class="math inline">\(\delta_t^{&#39;}M_t\delta_t &gt; 0\)</span>. Além disso, na situação de <span class="math inline">\(\psi(\pmb\theta)\)</span> não encontar-se no ponto máximo, como mencionado no parágrafo anterior, sempre haverá um tamanho de passo <span class="math inline">\(\lambda_t\)</span>, por menor que ele seja.</p>
<div id="steepest-ascent" class="section level4">
<h4><span class="header-section-number">6.2.1.1</span> Steepest Ascent</h4>
<p>O algoritmo <strong>steepest ascent</strong> (“subida mais inclinada”) é o mais simples dos métodos gradientes. A ideia do algoritmo <strong>steepest ascent</strong> é considerar</p>
<p><span class="math display">\[M_t = I,\]</span>
em que <span class="math inline">\(I\)</span> é a matriz identidade (<span class="math inline">\(p \times p\)</span>). Dessa forma, temos que <span class="math inline">\(\pmb \Delta_t = \delta_t\)</span>, em todos os passos iterativos. Daí, a atualização de <span class="math inline">\(\pmb \theta\)</span> é dada por:</p>
<p><span class="math display">\[\pmb \theta_{t+1} = \theta + \lambda_t\delta_t.\]</span></p>
<p><strong>Nota</strong>:</p>

<div class="rmdnote">
<div class="text-justify">
<p>Esse algoritmo tende a ser pouco utilizado na prática, uma vez que apresenta convergência lenta. Esse método é muito semelhante ao algoritmo gradiente descendente <a href="https://en.wikipedia.org/wiki/Gradient_descent"><strong>steepest descent</strong></a>, utilizado para minimização. Não confunda <strong>steepest descent</strong> citado aqui com um método de aproximação de integrais que leva o mesmo nome.</p>
</div>
</div>

</div>
<div id="newton-raphson" class="section level4">
<h4><span class="header-section-number">6.2.1.2</span> Newton-Raphson</h4>
<p>O método de <strong>Newton-Raphson</strong>, muitas vezes apenas chamado de método de <strong>Newton</strong>, considera a atualização de <span class="math inline">\(\pmb \theta\)</span> na forma</p>
<p><span class="math display">\[\pmb \theta_{t+1} = \pmb \theta_t -H_t^{-1}\delta_t,\]</span>
em que</p>
<p><span class="math display">\[H = H(\theta) = \frac{\partial^2 \psi(\pmb \theta)}{\partial \pmb \theta \partial \pmb \theta^{&#39;}},\]</span>
ou seja, temos que <span class="math inline">\(H\)</span> é a matriz <a href="https://pt.wikipedia.org/wiki/Hessiano"><strong>hessiana</strong></a>. No método de <strong>Newton-Rapshon</strong>, temos que <span class="math inline">\(\lambda_t = 1\)</span> e <span class="math inline">\(M_t = -H_t^{-1}\)</span>, em todas iterações.</p>
<p><strong>Observação</strong>:</p>

<div class="rmdobservation">
<div class="text-justify">
<p>A matriz hessiana nos permite identificar a concavidade de uma função multiparamétrica, desde que estas sejam duplamente diferenciável. Em um problema de maximização de uma função objetivo, temos que a matriz hessiana é negativa definida em uma região muito próxima ao ponto que maximiza a função. Dessa forma, <span class="math inline">\(-H^{-1}\)</span> é uma matriz positiva definida na mesma região, isto é, se <span class="math inline">\(\pmb a\)</span> é um vetor qualquer (<span class="math inline">\(p \times 1\)</span>) e <span class="math inline">\(H^{-1}\)</span> é uma matriz (<span class="math inline">\(p \times p\)</span>), então <span class="math inline">\(-\pmb a^{&#39;} H^{-1} \pmb a &lt; 0\)</span>.</p>
<p>Seja <span class="math inline">\(H\)</span> uma matriz hessiana como apresentada acima. Abaixo estão listadas algumas propriedades da de <span class="math inline">\(H\)</span>. Dessa forma, Então</p>
<ul>
<li><p><span class="math inline">\(\pmb a^{&#39;} H \pmb a &gt; 0\)</span> (positiva-definida): A função é estritamente convexa;</p></li>
<li><p><span class="math inline">\(\pmb a^{&#39;} H \pmb a &lt; 0\)</span> (negativa-definida): A função é estritiamente concava. Essa é a propriedade que estamos considerando, visto que estamos em um problema de maximização de uma função objetivo;</p></li>
<li><p><span class="math inline">\(\pmb a^{&#39;} H \pmb a \geq 0\)</span> (positiva-definida): A função é parcialmente convexa;</p></li>
<li><p><span class="math inline">\(\pmb a^{&#39;} H \pmb a \leq 0\)</span> (positiva-definida): A função é parcialmente concava;</p></li>
<li><p><span class="math inline">\(H\)</span> é uma matriz simétrica;</p></li>
<li><p>A inversa de uma matriz positiva-definida é também uma matriz positiva-definida;</p></li>
<li><p>Você poderá entender a matriz hessiana como a primeira derivada do vetor gradiente da função objetivo.</p></li>
</ul>
</div>
</div>

<p>O método de <strong>Newton-Raphson</strong> normalmente é utilizado para encontar os zeros de uma função. Quando aplicado sobre a condição de primeira ordem, o método de <strong>Newton-Raphson</strong> nos conduzirá aos pontos críticos da função. Como fazemos uso do vetor gradiente, estaremos sempre a caminhar para os pontos de máximo de <span class="math inline">\(\psi(\cdot)\)</span>. Dessa forma, os pontos críticos são os pontos de máximo da função objetivo.</p>
<p>Para que possamos entender melhor a ideia do método, considere uma <a href="https://pt.wikipedia.org/wiki/S%C3%A9rie_de_Taylor"><strong>expansão de Taylor</strong></a> da condição de primeira ordem em torno de um ponto qualquer <span class="math inline">\(\pmb \theta_0\)</span>. Assim, temos que</p>
<p><span class="math display">\[\frac{\partial\,\psi(\pmb \theta)}{\partial\, \pmb \theta} \approx f(\pmb \theta_0) + H(\pmb \theta_0) (\pmb \theta - \pmb\theta_0).\]</span></p>
<p>Resolvendo o sistema para <span class="math inline">\(\pmb\theta\)</span> e fazendo <span class="math inline">\(\pmb \theta = \pmb \theta_{t+1}\)</span> e <span class="math inline">\(\pmb \theta_0 = \pmb \theta_t\)</span>, obteremos o esquema iterativo do algoritmo apresentado ascima.</p>
<p>A forma mais usual do algoritmo de <strong>Newton-Rapshon</strong> introduz o mecanismo de “procura em linha”. Dessa forma, o esquema iterativo é dada por</p>
<p><span class="math display">\[\pmb \theta_{t+1} = \pmb \theta_t -\lambda_tH_t^{-1}\delta_t.\]</span>
<strong>Nota</strong>:</p>

<div class="rmdnote">
<div class="text-justify">
<p>O método de <strong>Newton-Raphson</strong> apesar de funcionar bem em diversas situações, poderá fornecer estimativas ruins em alguns casos. Por exemplo, um problema comum é quando <span class="math inline">\(\pmb \theta_t\)</span> é um ponto distante do ponto que maximiza <span class="math inline">\(\psi(\cdot)\)</span>, uma vez que nesses pontos, a matriz <span class="math inline">\(M_t = -H^{-1}\)</span> pode não ser positiva-definida.</p>
</div>
</div>

</div>
<div id="bhhh" class="section level4">
<h4><span class="header-section-number">6.2.1.3</span> BHHH</h4>
<p>O método <strong>BHHH</strong> (<strong>B</strong>erndt-<strong>H</strong>all-<strong>H</strong>all-<strong>H</strong>ausman), foi proposto no artigo Berndt, E. R., Hall, B. H., Hall, R. E., &amp; Hausman, J. A. (1974). <strong>Estimation and inference in nonlinear structural models</strong>. In Annals of Economic and Social Measurement, Volume 3, p. 653-665. O método é muito semelhante ao método de Newton-Raphson, com a diferença que trocamos a matriz <span class="math inline">\(H_t\)</span> pelo auto produto dos vetores gradientes <span class="math inline">\(\delta_t\delta_t^{&#39;}\)</span></p>
<p><strong>Nota</strong>:</p>

<div class="rmdnote">
<div class="text-justify">
<p>A vantagem do método <strong>BHHH</strong> está no fato de não necessitarmos calcular segundas derivadas. Trata-se de um método muito utilizado em aplicações ecnonométricas. O método <strong>BHHH</strong> poderá enfrentar o mesmo problema de convergência que o método de <strong>Newton-Raphson</strong>.</p>
</div>
</div>

</div>
<div id="metodos-quasi-newton" class="section level4">
<h4><span class="header-section-number">6.2.1.4</span> Métodos quasi-Newton</h4>
<p>A classe de algoritmos quasi-Newton é composta por algoritmos que são bastante eficientes do ponto de vista de convergência e também do ponto de vista computacional, visto que esses algoritmos não requerem o cálculo de segundas derivadas. Nessa classe de algoritmos, é utilizado uma sequência de matrizes tal que</p>
<p><span class="math display">\[M_{t+1} = M_{t} + N_t,\]</span>
sendo <span class="math inline">\(N_t\)</span> uma matriz positiva-definida. Assim, se <span class="math inline">\(M_0\)</span> é positiva-definida, <span class="math inline">\(M_t\)</span> na <span class="math inline">\(t\)</span>-ésima iteração sempre será positiva-definida. A ideia básica desses métodos é construir uma aproximação para <span class="math inline">\(-H(\pmb \theta)^{-1}\)</span>, de tal forma que</p>
<p><span class="math display">\[\lim_{t \to \infty} M_t = -H^{-1}.\]</span></p>
<p>Há diversos algoritmos que pertencem à classe de métodos quasi-Newton. Por exemplo, o método <strong>DFP</strong> (<strong>D</strong>avidon, <strong>F</strong>letcher e <strong>P</strong>owell) atualiza <span class="math inline">\(M_{t+1}\)</span> fazendo</p>
<p><span class="math display">\[M_{t+1} = M_{t} + \frac{\pmb \gamma_t \pmb \gamma_t^{&#39;}}{\pmb \gamma_t^{&#39;}\kappa_t} + \frac{M_t\kappa_t\kappa_t^{&#39;}M_t}{\kappa_t^{&#39;}M_t\pmb \gamma_t},\]</span></p>
<p>em que <span class="math inline">\(\pmb \gamma_t = \pmb \theta_{t+1} - \pmb \theta_t\)</span> (diferença de pontos) e <span class="math inline">\(\kappa_t = \delta(\pmb \theta_{t+1}) - \delta(\pmb \theta_t)\)</span> (diferenças de gradientes avaliados em pontos).</p>
<p>O algoritmo quasi-Newton mais utilizado é o <strong>BFGS</strong> (<strong>B</strong>royden-<strong>F</strong>letcher-<strong>G</strong>oldfarb-<strong>S</strong>hanno). O algoritmo <strong>BFGS</strong> tem a atualização semelhante ao <strong>DFP</strong>, com a diferença que é subtraído o termo <span class="math inline">\(a_tb_tb_t^{&#39;}\)</span>, em que <span class="math inline">\(a_t = \kappa_t^{&#39;}M_t\kappa_t\)</span> e</p>
<p><span class="math display">\[b_t = \frac{\pmb\gamma_t}{\pmb\gamma_t^{&#39;}\kappa_t} - \frac{M_t\kappa_t}{\kappa_t^{&#39;}M_t \kappa_t}.\]</span></p>
<p>Sendo assim, temos que no método <strong>BFGS</strong>, a atualização de <span class="math inline">\(M_t\)</span> de dará por:</p>
<p><span class="math display">\[M_{t+1} = M_{t} + \frac{\pmb \gamma_t \pmb \gamma_t^{&#39;}}{\pmb \gamma_t^{&#39;}\kappa_t} + \frac{M_t\kappa_t\kappa_t^{&#39;}M_t}{\kappa_t^{&#39;}M_t\pmb \gamma_t} - a_tb_tb_t^{&#39;}.\]</span></p>
<p><strong>Nota</strong>:</p>

<div class="rmdnote">
<div class="text-justify">
<p>O termo quasi-Newton é empregado para se referir ao fato de que esses métodos não fazem uso da matriz hessiana. Porém, esses métodos utilizam uma aproximação iterativa de uma matriz <span class="math inline">\(M_t\)</span> que converge para a matriz de segundas derivadas. Dessa forma, não entenda o termo quasi-Newton como se esses métodos fossem inferiores aos métodos de <strong>Newton-Raphson</strong>. Na verdade, os métodos quasi-Newton normalmente apresentam desempenho superior.</p>
</div>
</div>

<!-- #### Otimização não-linear no R -->
<!-- Em  R, é comum minimizar uma função objetivo utilizando a função `optim()` do pacote **stats** que está disponível em qualquer instalação básica da linguagem. A forma geral de uso da função `optim()` é: -->
<!-- ```{r, eval = FALSE} -->
<!-- optim(par, fn, gr = NULL, ..., -->
<!--       method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", -->
<!--                  "Brent"), -->
<!--       lower = -Inf, upper = Inf, -->
<!--       hessian = FALSE) -->
<!-- ``` -->
<!-- em que: -->
<!--    1. `par` é um vetor de chutes inicias; -->
<!--    2. `fn` é a função objetivo a ser **minimizada**; -->
<!--    3. `gr` é a função gradiente da função `fn`; -->
<!--    4. `method` é o método escolhido par minimizar `fn`. É possível escolher os métodos de Nelder-Mead, BFGS, CG, L-BFGS-B, SANN (Simulated Annealing) e Brent; -->
<!--    4. `...` é o operador dot-dot-dot que poderá receber argumentos que por ventura possam existir nas funções `fn` e `gr`; -->
<!--    5. `lower` é um vetor que limita inferiormente os parâmetros a serem otimizados (por padrão é `-Inf`); -->
<!--    6. `upper` é um vetor que limita superiormente os parâmetros a serem otimizados (por padrão é `Inf`); -->
<!--    7. `hessian` recebe um valor lógico, em que por padrão `hessian = FALSE`. Se `hessian = TRUE`, será calculado uma estimativa da matriz hessiana avaliada na estimativa de ponto de mínimo global. -->
<!-- **Exemplo**: Utilizando a função `optim()` para minimizar a função [**Rosenbrock**](https://en.wikipedia.org/wiki/Rosenbrock_function) pelo método BFGS. -->
<!-- ```{r, echo = FALSE, fig.align='center', fig.cap="Gráfico da superfície da função Rosenbrock introduzida por Howard H. Rosenbrock em 1960."} -->
<!-- rosenbrock <- function(x1, x2) (1 - x1) ^ 2 + 100 * (x2 - x1 ^ 2) ^ 2 -->
<!-- M  <- plot3D::mesh(seq(-2,  2, length.out = 550), -->
<!--                     seq(-2,  2, length.out = 550)) -->
<!-- x <- M$x -->
<!-- y <- M$y -->
<!-- z <- rosenbrock(x, y) -->
<!-- plot3D::surf3D(x, y, z, inttype = 1, bty = "b2", phi = 5, theta = 150) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- # Função Rosenbrock -->
<!-- f_objetivo <- function(x, a = 2, b = 40) {    -->
<!--     x1 <- x[1] -->
<!--     x2 <- x[2] -->
<!--     (a - x1) ^ 2 + b * (x2 - x1 ^ 2) ^ 2 -->
<!-- }    -->
<!-- optim(par = c(2, 2), fn = f_objetivo, method = "BFGS") -->
<!-- ``` -->
<!-- **Entendendo a saída (retorno da função)**: -->
<!--   1. `par` é um vetor contendo as estimativas para $x_1$ e $x_2$; -->
<!--   2. `value` é o valor da função avaliada nas estimativas obtidas; -->
<!--   3. `counts` é a quantidade de chamadas da função a ser otimizada e do seu gradiente. Note que não passamos o gradiente da função de objetivo mas este teve que ser calculado numericamente; -->
<!--   4. `convergence` é um vetor de uma única posição, em que 0 indica convergência. Qualquer número diferente de zero indica que houve problema de convergência no algoritmo escolhido para mininizar a função objetivo; -->
<!--   5. `message` retorna alguma mensagem com informações adicionais, se necessário. Caso contrário, `NULL` será o retorno. -->
<!-- A função de Rosenbrock tem mínimo analítico no ponto $(a, a^2)$ (ponto ótimo), em que $f(a, a^2) = 0$. Dessa forma, um bom método de otimização nos trará uma estimativa próxima ao ponto $(2, 4)$, uma vez que foi considerado, no código acima, $a = 2$ e $b = 40$. Como pode-se observar, o método BFGS forneceu uma boa estimativa para ponto ótimo analítico. -->
<!-- **Nota**: -->
<!-- ```{block2, type='rmdnote'} -->
<!-- <div class=text-justify> -->
<!-- Para o exemplo acima, $x_1$ e $x_2$ são chamadas de **variáveis**, visto que a função de Rosenbrock é determinística. Porém, em problemas estatísticos, por exemplo, quando desejamos maximizar uma função de log-verossimilhança, a otimização é realizada em termos dos **parâmetros** que indexam o modelo estatístico. Nesse caso, cada ponto da função  -->
<!-- </div> -->
<!-- ``` -->
<!-- O argumento `...` é muito importante para nós que necessitamos frequentemente maximizar uma função de log-vessimilhança $\mathcal{l}(\pmb \theta)$. É por meio desse argumento que passamos a amostra para $\mathcal{l}(\pmb \theta)$. Sem ele, teríamos que implementa uma função de verossimilhança para cada tamanho de amostra, o que seria impraticável. Com a amostra especificada, poderemos proceder a minimização de $-\mathcal{l}(\pmb \theta)$ e obter o vetor $\pmb \theta$ com as estimativas de máxima verossimilhança dos parâmetros que indexam um modelo probablístico. -->
<!-- **Exemplo**: Considere o conjunto de dados obtido abaixo: -->
<!-- ```{r, eval = FALSE, cache = TRUE, warning = FALSE} -->
<!-- set.seed(0L) -->
<!-- dados <- rnorm(n = 750L, mean = 2, sd = 1) -->
<!-- ``` -->
<!-- Seja $X_1, \ldots, X_n$ uma amostra aleatória (v.a.'s i.i.d), em que $X_i \sim \mathcal{N}(\mu = 2, \sigma^2 = 1)\, \forall i$. Obtenha pelo método BFGS os estimadores de máxima verossimilhança para $\mu$ e $\sigma^2$. Seja $\mathcal{l(\pmb\theta)}$ a função de verossimilhança para amostra e $\pmb \theta^{'} = (\mu, \sigma^2)$. Precisaremos implementar a função de verossmilhança e multiplica-la por -1, uma vez que minimizando $-\mathcal{l}(\pmb \theta)$ equivale à maximizar $\mathcal{l}(\pmb \theta)$. -->
<!-- ```{r, warning = FALSE, fig.align = 'center'} -->
<!-- set.seed(0L) -->
<!-- dados <- rnorm(n = 750L, mean = 2, sd = 1) -->
<!-- # Função de log-verssimilhança da amostra aleatória. -->
<!-- loglikelihood_normal <- function(par, x){ -->
<!--   mu <- par[1] -->
<!--   sigma2 <- par[2] -->
<!--   -sum(log(dnorm(x, mean = mu, sd = sqrt(sigma2)))) -->
<!-- } -->
<!-- # maximizando -log-likelihood da amostra aleatória. -->
<!-- resultado <- optim(par = c(1, 1), fn = loglikelihood_normal, -->
<!--                    method = "BFGS", x = dados) -->
<!-- # Graficando a densidade estimada sobre os dados ---------------------------------- -->
<!-- # Sequência no domínio da distribuição: -->
<!-- x <- seq(-5, 6, length.out = 1000L)  -->
<!-- y <- dnorm(x = x, mean = resultado$par[1], resultado$par[2]) -->
<!-- # Histograma das observações: -->
<!-- hist(x = dados, main = "Função Densidade de Probabilidade Estimada", -->
<!--      ylab = "Probabilidade", xlab = "x", probability = TRUE,  -->
<!--      col = rgb(1, 0.9, 0.8), border = NA) -->
<!-- # Tracejando a densidade sobre o histograma: -->
<!-- lines(x, y, lwd = 2) -->
<!-- ``` -->
<!-- **Importante**: -->
<!-- ```{block2, type='rmdimportant'} -->
<!-- <div class=text-justify> -->
<!-- Nunca esqueça que minimizar $-f$ é equivalente a maximizar $f$, sendo tabém verdadeira a recíproca. Além disso,  lembre-se também que a função `optim()` é implementada de forma a minimizar uma função objetivo. -->
<!-- </div> -->
<!-- ``` -->
<!-- ## Exercício {-} -->
<!-- 1. Defina matematicamente o método gradiente para maximização de uma função $\psi(\pmb{\theta}): \pmb{\Theta} \rightarrow  \mathbb{R}$, em que $\pmb{\Theta}$ é um subespaço do $\mathbb{R}^p$. -->
<!-- 2. Qual a desvantagem do uso do processo de busca em linha em algoritmos de otimização não-linear? -->
<!-- 3. Defina os métodos quasi-Newton e o que os diferenciam dos de Newton? Qual as vantagens que a maioria desses métodos apresentam com relação aos métodos de Newton? -->
<!-- 4. Enuncie os métodos **Steepest Ascent**, **Newton-Raphson** e **BHHH**. Esses são métodos de Newton ou quasi-Newton? Explique. -->
<!-- 5. Considere a [**Matyas function**](https://en.wikipedia.org/wiki/File:Matyas_function.pdf), função definida logo abaixo: -->
<!--     $$f(x, y) = 0.26 \times (x^2 + y^2) - 0.48 \times xy,$$ -->
<!--     em que $0 \leq x, y ,\leq 10$. Qual o ponto de mínimo global da função? Obtenha uma estimativa, utilizando o método BFGS, para o ponto de mínimo global da função. Além disso, construa o gráfico da superfície obtida pela função e o gráfico com as curvas de níveis da função com a estimativa obtida indicada como um ponto nesse último gráfico. Interprete o resultado obtido. Houve convergência? -->
<!-- 6. Considere a [**Himmelblau's function**](https://en.wikipedia.org/wiki/Test_functions_for_optimization#/media/File:Himmelblau_function.svg), função definida por: -->
<!--    $$f(x, y) = (x^2 + y -11)^2 + (x + y^2 - 7)^2,$$ -->
<!--    com $-5 \leq x,y \leq 5$. Essa função possui quatro pontos de mínimo global, são eles: -->
<!--    $$ -->
<!--    \mathrm{Min} = \left \{ -->
<!--    \begin{array}{rcc} -->
<!--    f(3.0, 2.0) & = & 0.0 \\ -->
<!--    f(-2.805118, 3.131312) & = & 0.0 \\ -->
<!--    f(-3.779310, -3.283186) & = & 0.0 \\ -->
<!--    f(3.584428, -1.848126) & = & 0.0 -->
<!--    \end{array}  -->
<!--    \right. -->
<!--    $$ -->
<!--    Obtenha uma estimativa, utilizando o método BFGS, para o ponto de mínimo global da função. Além disso, construa o gráfico da superfície obtida pela função e o gráfico com as curvas de níveis da função com a estimativa obtida indicada como um ponto nesse último gráfico. Interprete o resultado obtido. Houve convergência? -->

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="r-miscelanea-e-topicos-avancados.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/prdm0/aulas_computacional/edit/master/topicos_estatistica_computacional.Rmd",
"text": "Editar"
},
"history": {
"link": null,
"text": null
},
"download": ["aulas_r.pdf"],
"toc": {
"collapse": null
},
"toolbar": {
"position": "fixed",
"download": ["aulas_r.pdf", "Aulas de R"],
"search": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
